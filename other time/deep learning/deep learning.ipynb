{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward propagation with activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9901095378334199\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "input_data = np.array([-1,2])\n",
    "weights = {\n",
    "    'node 0':np.array([3,3]),\n",
    "    'node 1':np.array([1,5]),\n",
    "    'output':np.array([2,-1])\n",
    "}\n",
    "\n",
    "\n",
    "node_0_input = (input_data * weights['node 0']).sum()\n",
    "node_0_output = np.tanh(node_0_input)\n",
    "\n",
    "node_1_input = (input_data * weights['node 1']).sum()\n",
    "node_1_output = np.tanh(node_1_input)\n",
    "\n",
    "\n",
    "hidden_layer_outputs = np.array([node_0_output,node_1_output])\n",
    "\n",
    "\n",
    "\n",
    "output= (hidden_layer_outputs*weights['output']).sum()\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[30 40]\n",
      "2.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "weights = np.array([1,2])\n",
    "input_data= np.array([3,4])\n",
    "target=6\n",
    "learning_rate=0.01\n",
    "preds = (weights*input_data).sum()\n",
    "err = preds-target\n",
    "print(err)\n",
    "gradient=2*input_data*err\n",
    "print(gradient)\n",
    "\n",
    "weights_updated=weights-learning_rate*gradient\n",
    "preds_updated=(weights_updated*input_data).sum()\n",
    "err_updated=preds_updated-target\n",
    "print(err_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deep learning with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "loadtxt() got an unexpected keyword argument 'delimeter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-85ad8ebf1c4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpredictors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predictors_data.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelimeter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mn_cols\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpredictors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: loadtxt() got an unexpected keyword argument 'delimeter'"
     ]
    }
   ],
   "source": [
    "#model specification- regression models\n",
    "\n",
    "from keras.models import Sequential\n",
    "predictors = np.loadtxt('predictors_data.txt',delimeter=',')\n",
    "n_cols= predictors.shape[1]\n",
    "model= Sequential()\n",
    "model.add(Dense(100, activation='relu',input_shape=(n_cols,)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "#compiling and fitting a model\n",
    "model.compile(optimizer='adam',loss='mean_squared_error')\n",
    "\n",
    "#fit model\n",
    "model.fit(predictors, target)\n",
    "\n",
    "#classification models\n",
    "from keras.utils import to_categorical\n",
    "data = pd.read_csv('basketball_shot_log.csv')\n",
    "predictors = data.drop(['shot_result'],axis=1).as_matrix()\n",
    "target = to_categorical(data.shot_result)\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu',input_shape=(n_cols,)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit(predictors,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.model import load_model\n",
    "\n",
    "model.save('model_file.h5')\n",
    "\n",
    "my_model = load_model('model_file.h5')\n",
    "\n",
    "predictions = my_model.predict(data_to_predict_with)\n",
    "\n",
    "probability_true = predictions[:,1]\n",
    "\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_model(input_shape =  input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation = ' relu', input_shape= input_shape))\n",
    "    model.add(Dense(100, activation= 'relu'))\n",
    "    model.add(Dense(2,activation = 'softmax'))\n",
    "    return model\n",
    "\n",
    "lr_to_test = [0.000001,0.01, 1]\n",
    "\n",
    "for lr in lr_to_test:\n",
    "    model = get_new_model()\n",
    "    my_optimizer = SGD(lr= lr)\n",
    "    model.compile(optimizer=my_optimizer, loss='categorical_crossentropy')\n",
    "    model.fit(predictors, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#model validation\n",
    "model.fit(predictors, target, validation_split =0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#early stopping\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience = 2)\n",
    "model.fit(predictors, target, validation_split=0.3, epochs= 20, callbacks =[early_stopping_monitor])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
